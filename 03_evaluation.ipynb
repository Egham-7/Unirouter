{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Results\n",
    "\n",
    "**Paper Metric**: Deferral curves showing cost vs accuracy tradeoff\n",
    "\n",
    "**Key Result**: Universal routing works with new models without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load router from previous notebook\n",
    "%run 02_model_characterization.ipynb"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation\n",
    "\n",
    "Independent test set (different from validation used for characterization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create test set\n",
    "test_dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "test_dataset = test_dataset.shuffle(seed=123).select(range(50))\n",
    "\n",
    "test_set = []\n",
    "for item in test_dataset:\n",
    "    prompt = f\"{item['question']}\\nA) {item['choices'][0]}\\nB) {item['choices'][1]}\\nC) {item['choices'][2]}\\nD) {item['choices'][3]}\\nAnswer:\"\n",
    "    test_set.append({\n",
    "        'prompt': prompt,\n",
    "        'answer': ['A', 'B', 'C', 'D'][item['answer']]\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Test set: {len(test_set)} examples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deferral Curve Analysis\n",
    "\n",
    "**Core Experiment**: How does accuracy change with cost as we vary Î»?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "lambda_values = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "results = []\n",
    "\n",
    "for Î» in lambda_values:\n",
    "    correct, total_cost = 0, 0\n",
    "    \n",
    "    for example in test_set:\n",
    "        # Route and predict\n",
    "        routing = router.route(example['prompt'], lambda_cost=Î»)\n",
    "        selected = routing['model']\n",
    "        total_cost += routing['cost']\n",
    "        \n",
    "        response = call_llm(\n",
    "            selected,\n",
    "            router.model_db[selected]['provider'],\n",
    "            example['prompt']\n",
    "        )\n",
    "        \n",
    "        if response == example['answer']:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_set)\n",
    "    avg_cost = total_cost / len(test_set)\n",
    "    \n",
    "    results.append({'lambda': Î», 'accuracy': accuracy, 'cost': avg_cost})\n",
    "    print(f\"Î»={Î»:5.3f}: {accuracy:.1%} accuracy, ${avg_cost:.3f} avg cost\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deferral Curve Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "costs = [r['cost'] for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(costs, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "\n",
    "for r in results:\n",
    "    plt.annotate(f\"Î»={r['lambda']}\", (r['cost'], r['accuracy']), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Average Cost ($)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('UniRoute Deferral Curve\\n(Jitkrittum et al., 2025)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Experiment complete!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Models (Zero Retraining)\n",
    "\n",
    "**Paper's Key Advantage**: Add new models by just computing their Î¨(m) profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def add_new_model(model_name: str, provider: str, cost: float):\n",
    "    \"\"\"Add model without retraining router\"\"\"\n",
    "    \n",
    "    # Compute error profile on existing clusters\n",
    "    psi_vector = characterize_model(model_name, provider)\n",
    "    \n",
    "    # Add to database\n",
    "    router.model_db[model_name] = {\n",
    "        'psi_vector': psi_vector,\n",
    "        'provider': provider,\n",
    "        'cost': cost\n",
    "    }\n",
    "    \n",
    "    router._normalize_costs()\n",
    "    print(f\"âœ… {model_name} added!\")\n",
    "\n",
    "# Example: Add better model\n",
    "# add_new_model('llama-3.3-70b-versatile', 'groq', 0.69)\n",
    "\n",
    "print(\"ðŸ’¡ New models can be added dynamically!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights (Jitkrittum et al., 2025)\n",
    "\n",
    "- âœ… **Universal**: Works with any new LLM\n",
    "- âœ… **Efficient**: No router retraining needed\n",
    "- âœ… **Practical**: Real cost-quality optimization\n",
    "- âœ… **Scalable**: O(K) cost to add new models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}